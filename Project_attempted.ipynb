{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNy8ouNLvk8I"
      },
      "source": [
        "# SNLP Project\n",
        "\n",
        "Name 1: Viet Dao Quoc <br/>\n",
        "Student id 1: 7010834 <br/>\n",
        "Email 1: vida00001@stud.uni-saarland.de <br/>\n",
        "\n",
        "Name 2: Angelin Mary Jose <br/>\n",
        "Student id 2: 7029912 <br/>\n",
        "Email 2: anjo00002@stud.uni-saarland.de <br/>\n",
        "\n",
        "**Instructions:** Read each question carefully. <br/>\n",
        "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br/>\n",
        "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after your upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sgkw061ZpFs7"
      },
      "source": [
        "# Question 1: The Theory\n",
        "\n",
        "## GloVe (4 points)\n",
        "1.  Let's start with the original paper [\\[Link\\]](https://nlp.stanford.edu/pubs/glove.pdf). Read the paper and answer these questions: </br>\n",
        "  a. What do you think of the jump from (Equation 3 + Equation 4) -> Equation 5 ? Provide a counter-example where the given result may not hold. Why does this not affect the algorithm?  (2 points) </br>\n",
        "\n",
        "  b. Why does GloVe use a smoothing function ? How does the smoothing function interact with the loss objective? (1 point) </br>\n",
        "\n",
        "  c. Look at the [Mittens](https://arxiv.org/pdf/1803.09901.pdf) Paper, which extends GloVe. What do they add to the GloVe loss function? What happens as a result?(1 point)\n",
        "\n",
        "\n",
        "## Word2Vec (5 points)\n",
        "\n",
        "\n",
        "\n",
        "2. The two word2vec papers can be skimmed through to answer the following questions. [[Paper](https://arxiv.org/pdf/1301.3781.pdf)], \\[[2nd Paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\\] (Optional)]. You can also use [this](https://docs.chainer.org/en/latest/examples/word2vec.html) post for reference. Additionally, for an intuition behind vector operations check this [blog post](https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html/). The word2vec paper has a neural focus, however, it is not crucial to understand how neural networks work at this time. Answer the following questions: </br>\n",
        "\n",
        "  a. Describe the two proposed methods/training objectives for obtaining word embeddings presented in the original paper. Focus only on the **training objective** and not the technical/optimization details. (2 points) </br>\n",
        "\n",
        "  b. In section 2.1 of the paper, the authors mention using a hierarchical softmax function where they represent the vocabulary as a Huffman binary tree. They go on to claim that only $\\log_2(Unigram\\_perplexity(V))$ evaluations are needed to arrive at a result, where $V$ is the size of the vocabulary. Why is this the case? (1 point) </br>\n",
        "\n",
        "  c. An extension of the skip-gram approach to obtaining word2vec embeddings. This extension is called skip-gram with negative sampling. It is described in the second paper. Explain what this method consists in and why it is more efficient. (1 point) </br>\n",
        "\n",
        "  d. Attempts at explaining what word2vec is doing have tried to link the resulting $W$ embedding matrix to the good ol' point-wise mutual information (PMI) approaches. Explain why this might make sense as an intuition. Feel free to answer with what has been covered in the SNLP course or take a look at this [paper](https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf) which claims that $WC$ is the PMI shifted by a constant, where $W$ is our embedding matrix and $C$ is our context matrix. (1 point)\n",
        "\n",
        "**Notes**:\n",
        "* (Question 2) In a neural language model, we predict the probability of each token in the vocabulary to be the next token given the context. In order to get this distribution we use the $softmax(x) = \\frac{\\exp(x)}{\\sum_{i=0}^V \\exp(w_i)}$ function. It is not completely necessary to understand this fully. Hint: If you wish to learn more about this, feel free to take the Neural Networks course.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Answer:\n",
        "+ Glove:\n",
        "    1. Since we have the assumption that F is a homomorphism then have have:\n",
        "    $$F(A+B) = F(A)*F(B)$$ \n",
        "    <br/>\n",
        "    \n",
        "    Apply the function above to left side of Eqn.3:\n",
        "    $$F(w^T_{i}.\\tilde{w}_{k} - w^T_{j}.\\tilde{w}_{k})$$\n",
        "    $$= F(w^T_{i}.\\tilde{w}_{k})* F(-w^T_{j}.\\tilde{w}_{k})$$\n",
        "    $$= \\frac{F(w^T_{i}.\\tilde{w}_{k})}{F(w^T_{j}.\\tilde{w}_{k})}$$\n",
        "    Then we have:\n",
        "    $$F(w^T_{i}.\\tilde{w}_{k} - w^T_{j}.\\tilde{w}_{k}) = \\frac{F(w^T_{i}.\\tilde{w}_{k})}{F(w^T_{j}.\\tilde{w}_{k})} = \\frac{P_{ik}}{P_{jk}}$$\n",
        "    Then implies:\n",
        "    $$F(w^T_{i}.\\tilde{w}_{k}) = P_{ik}$$ \n",
        "    and \n",
        "    $$F(w^T_{j}.\\tilde{w}_{k}) = P_{jk} $$\n",
        "    \n",
        "    <br/>\n",
        "\n",
        "    It is mentioned that the approximation introduced by Equation 5 may not hold perfectly in\n",
        "    practice.The authors contend that the performance of the algorithm is not materially\n",
        "    affected by the gap between the approximation and the actual goal function. This is\n",
        "    because GloVe's optimisation method modifies word representations depending on the\n",
        "    corpus's overall statistics, making it resistant to modest approximation-related errors.\n",
        "    \n",
        "    2. With the expression in the Eqn.6, it becomes undefined when $$X_{ij} = 0$$ But the paper later use Laplace smoothing technique but this results in the equal weightage of all word occurrences => weighted Least Square loss function. GloVe uses this smoothing function to mitigate the impact of very frequent words on the\n",
        "    learning process. GloVe reduces the influence of these highly frequent words which\n",
        "    could occur mostly in large corpora where certain words tend to co-occur in almost all\n",
        "    contexts,by still considering their co-occurrence information.\n",
        "    Through the weighted least squares regression model (Equation 4) used to approximate\n",
        "    the logarithm of the co-occurrence ratios, the smoothing function interacts with the loss\n",
        "    objective in GloVe. The smoothing function is applied to the co-occurrence counts,\n",
        "    modifying the weights used in the regression in this equation.\n",
        "    \n",
        "    <br/>\n",
        "\n",
        "    3. By adding a factor to the objective that penalises the squared Euclidean distance\n",
        "    between the learned embedding, wi=wi+wi, and an existing one, ri, the GloVe loss\n",
        "    function is here extended into a retrofitting model:\n",
        "    $$J_{Mittens} = J + \\mu * \\sum(w_{i} - r_{i})^2 $$ \n",
        "    \n",
        "    <br/>\n",
        "    \n",
        "    This term encourages word vectors to be sparse and have a greater number of zero\n",
        "    elements, which results in more comprehensible and compact representations.\n",
        "    The \"Mittens\" concept presents various implications and potential advantages as a result\n",
        "    of these additions:\n",
        "    Mittens selectively evaluates context words based on relatedness measurements in\n",
        "    order to learn word representations that are specialised to a particular area. When\n",
        "    dealing with tasks or data that are specialised to a certain domain and where it's crucial\n",
        "    to record accurate semantic relationships.\n",
        "\n",
        "+ Word2Vec: <br/>\n",
        "    1. <br/>\n",
        "        - Continuous Bag-of-Words (CBOW): The CBOW model aims to predict the target word based on its context. It treats the context words surrounding the target word as the input and tries to predict the target word itself. The context words are typically defined as a fixed-size window of words before and after the target word in a given text corpus. <br/>\n",
        "        - Skip-gram: The Skip-gram model, on the other hand, takes a target word as input and aims to predict the context words that are likely to appear around it. It flips the perspective of the CBOW model. Given a target word, the Skip-gram model tries to maximize the likelihood of generating the context words within a certain window around the target word. <br/>\n",
        "    \n",
        "    2. The claim that only log2(Unigram_perplexity(V)) evaluations are needed in the hierarchical softmax of word2vec is because the Huffman binary tree efficiently represents the word probabilities. The tree structure assigns shorter codes to more frequent words, reducing the average code length. As a result, the number of evaluations required is proportional to the average code length, which is logarithmically related to the unigram perplexity (a measure of word frequency distribution) of the vocabulary. Hence, the claim states that log2(Unigram_perplexity(V)) evaluations are sufficient to compute the probabilities, making the process more computationally efficient than evaluating all words individually. <br/>\n",
        "\n",
        "    3. The idea of negative sampling is to first get hold of positive samples and negative samples from the dataset given a specific center word. With this approach we can avoid back-propagating into a huge matrix at each window iteration which leads to a huge amount of computation, as it instead of considering all context words as positive examples, a small subset of negative examples is randomly selected from a noise distribution. These negative examples are unlikely to appear in the context.\n",
        "    - Computational Efficiency: Skip-gram with negative sampling simplifies the objective\n",
        "    function by only evaluating a small number of positive and negative examples, rather\n",
        "    than calculating softmax probabilities for the entire vocabulary. This reduces\n",
        "    computational complexity and speeds up training. \n",
        "    - Quality of Embeddings: By focusing on distinguishing between positive and negative\n",
        "    examples, skip-gram with negative sampling helps the model learn to better capture\n",
        "    word relationships. \n",
        "    \n",
        "    <br/>\n",
        "\n",
        "    4. The distributional hypothesis, which postulates that words with similar meanings tend to\n",
        "    have similar contexts, is supported by both word2vec and PMI techniques. Word2vec builds\n",
        "    distributed representations that capture comparable contextual patterns for words, whereas PMI\n",
        "    determines the strength of association between words based on their shared contexts. Both\n",
        "    strategies attempt to extract and vectorize semantic information by taking into account word\n",
        "    co-occurrence patterns.\n",
        "    The research hypothesises a relationship between the PMI shifted by a constant and the\n",
        "    word-context matrix product in word2vec. This link suggests that the contextual information in\n",
        "    the PMI values is captured by the word embeddings learnt by word2vec. It suggests that\n",
        "    word2vec effectively decomposes the PMI matrix into word and context embeddings, performing\n",
        "    a type of implicit matrix factorization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCwxrFzE50HK"
      },
      "source": [
        "# Question 2: Training GloVe Embeddings(11 points)\n",
        "\n",
        "Let's train our own GloVe embeddings!\n",
        "1. Start by splitting the corpus into train:test using a 50:50 ratio. Remove all punctuations and lowercase the corpus. (1 point)\n",
        "\n",
        "2. Write a function that computes the co-occurrence matrix for a fixed vocabulary and given window length. (2 points)\n",
        "\n",
        "3. Train your own GloVe embeddings from scratch using the [glove](https://github.com/stanfordnlp/glove) repo. Use the default parameters in ```demo.sh``` for this question. Check for empty and duplicate embeddings!\n",
        " (2 points)\n",
        "\n",
        "4. Use the resulting embeddings to train a sentiment classifier using your train data. Represent each sentence as the **sum** of its word vectors and train a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Replace all OOV entries with a zero vector. Use 5-fold cross validation on your training data to fix the depth of your Random forest. Keep all other hyperparameters unchanged. Print the F1 score (macro) for your classifier. (2 points)\n",
        "\n",
        "5. Plot the classifier performance across a basket of  vector embedding sizes {50,100,200} + {300 if you can} and context lengths [5,10,15] for developing the embeddings. What do you think about the trends? (1 point)\n",
        "\n",
        "\n",
        "6. Replace your embeddings with the global GloVe embeddings [\\[Link\\]](https://glove-embeddings.github.io/) (Mind the vocabularies). Re-run the classifier training and testing. How does the results  compare to your in-house embeddings? Plot the classifier performance against vector length [50,100,200] for the in-house and global embeddings.  (2 points)\n",
        "\n",
        "7. Let's see if the trends you observed hold across domains. Compare the IMDB embeddings you built against the global embeddings (50 sized vectors) when building a similar classifier for the [Financial Phrasebank corpus](https://huggingface.co/datasets/financial_phrasebank). What do you observe? (1 point)\n",
        "\n",
        "\n",
        "Note: The default setting for Glove is a context length of 10 and an embedding size of 50. If you are experimenting with one of the variables, you can default the other one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyfOQO0d6C0V"
      },
      "source": [
        "## Downloading data, Glove repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XsvcFCi5zpK"
      },
      "outputs": [],
      "source": [
        "!git clone http://github.com/stanfordnlp/glove\n",
        "!cd glove && make;\n",
        "!wget https://github.com/Ankit152/IMDB-sentiment-analysis/raw/master/IMDB-Dataset.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yto4vOGebpvM"
      },
      "source": [
        "## Into the Void . . ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9AdQnfsFbryz"
      },
      "outputs": [],
      "source": [
        "#Write your code here.\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('IMDB-Dataset.csv')\n",
        "def split_and_preprocess_data(data):\n",
        "    data['review'] = data['review'].map(lambda x: x.lower())\n",
        "    data['review'] = data['review'].apply(lambda x: re.sub(r'[^\\w\\s]', '',x))\n",
        "    return data.iloc[:int(data.shape[0]/2)], data.iloc[int(data.shape[0]/2):]\n",
        "\n",
        "\n",
        "def generate_co_occurrence_matrix(corpus, window_size):\n",
        "    co_occurrences = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    new_corpus = corpus\n",
        "    new_corpus['tokens'] = new_corpus['review'].apply(lambda row: word_tokenize(row))\n",
        "    new_corpus['unigrams'] = new_corpus['review'].apply(lambda row: list(nltk.ngrams(row.split(), 1)))\n",
        "    for doc in corpus:\n",
        "        vectorizer = CountVectorizer(binary=True)\n",
        "        occurrence_matrix = vectorizer.fit_transform([doc])\n",
        "\n",
        "        feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "        for i, word in enumerate(feature_names):\n",
        "            context_indices = np.where(occurrence_matrix.toarray()[0] == 1)[0]\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(feature_names), i + window_size + 1)\n",
        "            context_indices = context_indices[(context_indices >= start) & (context_indices < end)]\n",
        "\n",
        "            for j in context_indices:\n",
        "                context_word = feature_names[j]\n",
        "                co_occurrences[word][context_word] += 1\n",
        "\n",
        "    matrix = pd.DataFrame(co_occurrences).fillna(0)\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT4XS859w4V_"
      },
      "source": [
        "### GloVE training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHZ003SKh3Ar"
      },
      "outputs": [],
      "source": [
        "#Make edits to demo.sh before running!\n",
        "!cd glove && ./demo.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymMc1ub_fAtq"
      },
      "source": [
        "## Financial Phrasebank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsrUH0wxdXrl"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRth0jh4cuYr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"financial_phrasebank\", 'sentences_75agree')\n",
        "#1 - Neutral, 2- positive, 3 - negative\n",
        "\n",
        "test_financial=dataset['train'].to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFAFvzfS7Hcf"
      },
      "source": [
        "# Question 3: Training word2vec embeddings (10 points)\n",
        "\n",
        "Let's train our own word2vec embeddings!\n",
        "\n",
        "Some libraries are readily available to train your own word2vec embeddings. We're gonna keep it fun and you will implement some small intermediary code to gain further intuitions.\n",
        "\n",
        "1. Re-use the splits you used for the GloVe training. Alternatively, if you skipped the GloVe training, split the corpus into `train:test` using a `50:50` ratio. Remove all punctuations and lowercase the corpus.\n",
        "\n",
        "2. Implement a function that, given a window of words, a negative sample rate and a vocabulary, for every word generates training examples for a model with a negative example rate of `negative_sample_rate`. Note: you will **not** be using the resulting code of this exercise for further points. (3 points)\n",
        "\n",
        "2. Train your own word2vec embeddings from scratch using the [gensim](https://radimrehurek.com/gensim/models/word2vec.html) library. Perform 5-fold cross validation to find optimal values for the sampling rate, the length of the vector and the context size. (2 points)\n",
        "\n",
        "3. Use the resulting embeddings to train a sentiment classifier using your train data. Represent each sentence as the **sum** of word2vec vectors for the tokens in the text and train a random classifier (use the parameters from Q2.4; if you skipped that question use 5-fold cross validation for the tree depth). Handle OOV tokens with zero vectors. Print the F1 score (macro) for your classifier. (2 points)\n",
        "\n",
        "4. Now replace your embeddings with the pre-trained word2vec embeddings trained on Google News [[Link](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models)]. Re-run the classification experiments. Plot the classifier performance against vector length for the in-house and the Google News embeddings. To what do you attribute the shift in performance? (2 points)\n",
        "\n",
        "5. Compare the word2vec and GloVe embeddings you built. (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnH72ow47DiA"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install -q pandas gensim nltk tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTS2Lkfo7LpK"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "# Feel free to add more libraries here\n",
        "import re\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDvzEwo47N3J"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
        "    word_tokens = word_tokenize(text)\n",
        "    if remove_stopwords:\n",
        "        stop_words = stopwords.words('english')\n",
        "        tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "        processed_text = ' '.join(tokens)\n",
        "    return processed_text    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_iP34Q57PKv"
      },
      "outputs": [],
      "source": [
        "# Preprocess your data with the above method\n",
        "train = [preprocess_text(review, remove_stopwords=True) for review in train.review.tolist()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O561QxNq8sH"
      },
      "source": [
        "### Implement your own negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V6kNaNU6q6zg"
      },
      "outputs": [],
      "source": [
        "# Implement your own negative sampling method\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def negative_sampling(context: str,\n",
        "                      window_size: int,\n",
        "                      vocabulary: list,\n",
        "                      sampling_rate: float):\n",
        "    negative_samples = []\n",
        "\n",
        "    for positive_sample in context:\n",
        "        for _ in range(vocabulary):\n",
        "            negative_sample = None\n",
        "            while negative_sample is None or negative_sample in context:\n",
        "                negative_sample = random.choice(context)\n",
        "            negative_samples.append(negative_sample)\n",
        "\n",
        "    return negative_samples\n",
        "    \n",
        "\n",
        "def split_and_preprocess_data(data):\n",
        "    data['review'] = data['review'].map(lambda x: x.lower())\n",
        "    data['review'] = data['review'].apply(lambda x: re.sub(r'[^\\w\\s]', '',x))\n",
        "    return data.iloc[:int(data.shape[0]/2)], data.iloc[int(data.shape[0]/2):]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsSa7d-ArDgB"
      },
      "source": [
        "### Train your own embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2gEiW-gf7SlR"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "df1 = pd.read_csv('IMDB-Dataset.csv')\n",
        "word_freq = defaultdict(int)\n",
        "for sent in df1['review']:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "len(word_freq)\n",
        "train1, test1 = split_and_preprocess_data(df1)\n",
        "# Train your Word2Vec model\n",
        "    \n",
        "model = Word2Vec(sentences=train1['review'],\n",
        "                 vector_size=100, # Dimensionality of vectors\n",
        "                 min_count = 5, # Restricting vocabulary based on counts\n",
        "                 window = 5, # Window size\n",
        "                 max_vocab_size = None, # Restrict vocab size by fixed number\n",
        "                 sg = 0, # skip-gram\n",
        "                 hs = 0, # hierarchical softmax\n",
        "                 negative = 5, # use negative sampling and the rate\n",
        "                 compute_loss= True\n",
        "                )\n",
        "    \n",
        "model.build_vocab(df1['review'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7Pr786pk7UzT"
      },
      "outputs": [],
      "source": [
        "#check similar words to test\n",
        "test2 = test1.iloc[:1]\n",
        "for sentence in test2['review']:\n",
        "    for word in sentence:\n",
        "        if word in test1:\n",
        "            print(model.wv.most_similar(positive= test1, topn=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRFtgfTErWL2"
      },
      "source": [
        "### Use pre-trained embeddings"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "OT4XS859w4V_"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
